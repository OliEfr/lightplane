# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.


# [[[cog
#
# # example call: cog -d -D N_LAYERS_TRUNK=2 -D N_LAYERS_OPACITY=1 -D N_LAYERS_COLOR=1 COG_UTIL_MODULE=lightplane.triton_src.templates.cog_util ./mlp_util.py
# # This `cog` template expects the following constants:
# #  N_LAYERS_TRUNK: number of mlp layers in trunk
# #  N_LAYERS_OPACITY: number of mlp layers in opacity head
# #  N_LAYERS_COLOR: number of mlp layers in color head
# #  COG_UTIL_MODULE: the module name of the cog_util.py file
#
# import cog
# import importlib
# N_LAYERS_TRUNK, N_LAYERS_OPACITY, N_LAYERS_COLOR = int(N_LAYERS_TRUNK), int(N_LAYERS_OPACITY), int(N_LAYERS_COLOR)
# cog_util = importlib.import_module(COG_UTIL_MODULE)
#
# cog.outl('"""')
# cog.outl("===> This is templated code auto-generated by `cog`. Do not edit directly. <===")
# cog.outl("Templating parameters:")
# cog.outl(f"    N_LAYERS_TRUNK = {N_LAYERS_TRUNK}")
# cog.outl(f"    N_LAYERS_OPACITY = {N_LAYERS_OPACITY}")
# cog.outl(f"    N_LAYERS_COLOR = {N_LAYERS_COLOR}")
# cog.outl('"""')
# cog.outl("")
# cog.outl("import triton")
# cog.outl("import triton.language as tl")
# cog.outl("from ..shared.const import ALLOW_TF32")
# cog.outl("")
# cog.outl("@triton.jit")
# cog.outl('def mlp_trunk(x): pass')
# cog.outl("")
# cog.outl("@triton.jit")
# cog.outl(f"def load_weight_2dim(ptr, dim_in, dim_out):")
# cog.outl(f"   offs = tl.arange(0, dim_in)[:, None] * dim_out + tl.arange(0, dim_out)[None, :]")
# cog.outl(f"   w = tl.view(tl.load(ptr + offs), (dim_in, dim_out))")
# cog.outl(f"   return w")
# cog.outl("")
# cog.outl("@triton.jit")
# cog.outl(f"def load_weight_last_opacity(ptr, dim_in, BLOCK_SIZE):")
# cog.outl(f"   offs = tl.zeros((BLOCK_SIZE, 1), dtype=tl.int32) + tl.arange(0, dim_in)[None, :]")
# cog.outl(f"   w = tl.view(tl.load(ptr + offs), (BLOCK_SIZE, dim_in))")
# cog.outl(f"   return w")
# cog.outl("")
# cog.outl("@triton.jit")
# cog.outl("def load_bias_last_opacity(ptr, BLOCK_SIZE):")
# cog.outl("    return tl.view(tl.load(ptr + tl.zeros((BLOCK_SIZE,), dtype=tl.int32)), (BLOCK_SIZE,))")
# cog.outl("")
# cog.outl("@triton.jit")
# cog.outl(f"def load_weight(ptr, dim_in, dim_out):")
# cog.outl(f"   return load_weight_2dim(ptr, dim_in, dim_out)")
# cog.outl("")
# cog.outl("@triton.jit")
# cog.outl("def load_bias(ptr, dim, BLOCK_SIZE):")
# cog.outl("    return tl.view(tl.load((ptr + tl.arange(0, dim))[None, :] + tl.zeros((BLOCK_SIZE, 1), dtype=tl.int32)), (BLOCK_SIZE, dim))")
# cog.outl("")
# cog.outl("@triton.jit")
# cog.outl(f"def update_weight(ptr, dim_in, dim_out, grad):")
# cog.outl(f"    offs = tl.arange(0, dim_in)[:, None] * dim_out + tl.arange(0, dim_out)[None, :]")
# cog.outl(f"    tl.atomic_add(ptr + offs, grad)")
# cog.outl("")
# cog.outl("@triton.jit")
# cog.outl(f"def update_bias(ptr, dim, grad):")
# cog.outl(f"    offs = tl.arange(0, dim)")
# cog.outl(f"    tl.atomic_add(ptr + offs, grad)")
# cog.outl("")
# def create_load_mlp_params_def(mlp_name, n_layers):
#     mlp_name_cap = mlp_name.upper()
#     prev_offs = "0"
#     cog.outl("@triton.jit")
#     cog.outl(f"def load_mlp_params_{mlp_name}(mlp_params, DIM_IN, DIM_HIDDEN, DIM_OUT, BLOCK_SIZE):")
#     for load_weight in [True, False]:
#         dim_in = "DIM_IN"
#         for l in range(int(n_layers)):  # load weights
#             dim_out = f"DIM_OUT" if (l == int(n_layers) - 1) else f"DIM_HIDDEN"
#             if load_weight:
#                 cog.outl(f"    w{l}_offs = {prev_offs}")
#                 cog.outl(f"    w{l}_numel = {dim_in} * {dim_out}")
#                 if l==int(n_layers)-1 and (mlp_name=="opacity" or mlp_name=="OPACITY"):
#                     cog.outl(f"    w{l} = load_weight_last_opacity(mlp_params + w{l}_offs, {dim_in}, BLOCK_SIZE)")
#                 else:
#                     cog.outl(f"    w{l} = load_weight(mlp_params + w{l}_offs, {dim_in}, {dim_out})")
#                 prev_offs = f"w{l}_offs + w{l}_numel"
#             else:
#                 cog.outl(f"    b{l}_offs = {prev_offs}")
#                 cog.outl(f"    b{l}_numel = {dim_out}")
#                 if l==int(n_layers)-1 and (mlp_name=="opacity" or mlp_name=="OPACITY"):
#                     cog.outl(f"    b{l} = load_bias_last_opacity(mlp_params + b{l}_offs, BLOCK_SIZE)")
#                 else:
#                     cog.outl(f"    b{l} = load_bias(mlp_params + b{l}_offs, {dim_out}, BLOCK_SIZE)")
#                 prev_offs = f"b{l}_offs + b{l}_numel"
#             dim_in = dim_out
#     cog.outl(f"    return {prev_offs}, " +  cog_util.get_wb_str(None, n_layers))
#     cog.outl("")
#
# def create_mlp_def(mlp_name, n_layers):
#     wb_str = cog_util.get_wb_str(None, n_layers)
#     cog.outl("@triton.jit")
#     cog.outl(f"def mlp_{mlp_name}(x, " + wb_str + "):")
#     for l in range(int(n_layers)):
#         if (mlp_name=="opacity" or mlp_name=="OPACITY") and l==int(n_layers)-1:
#              cog.outl(f"    x = tl.sum(x * w{l}, axis=1) + tl.ravel(b{l})")
#         else:
#             cog.outl(f"    x = tl.dot(x, w{l}, allow_tf32=ALLOW_TF32) + b{l}")
#         if (l < int(n_layers) - 1) or (mlp_name == "trunk"):
#             cog.outl(f"    x = tl.maximum(x, 0.0)")
#     cog.outl(f"    return x")
#     cog.outl("")
#
# cog.outl("# We re-define another function for feedforward which stores intermediate results for backpropogation")
# def create_mlp_with_inter_feat_def(mlp_name, n_layers):
#     wb_str = cog_util.get_wb_str(None, n_layers)
#     cog.outl("@triton.jit")
#     cog.outl(f"def mlp_{mlp_name}_with_inter_feat(x, " + wb_str + "):")
#     inter_x = ""
#     inter_xwb = ""
#     for l in range(int(n_layers)):
#         cog.outl(f"    x{l} = x")
#         inter_x = inter_x + f", x{l}"
#         if (mlp_name=="opacity" or mlp_name=="OPACITY") and l==int(n_layers)-1:
#              cog.outl(f"    x = tl.sum(x * w{l}, axis=1)")
#              cog.outl(f"    x = x + tl.ravel(b{l})")
#         else:
#             cog.outl(f"    x = tl.dot(x, w{l}, allow_tf32=ALLOW_TF32) + b{l}")
#         if (l < int(n_layers) - 1) or (mlp_name == "trunk"):
#             cog.outl(f"    xwb{l} = x")
#             inter_xwb = inter_xwb + f", xwb{l}"
#             cog.outl(f"    x = tl.maximum(x, 0.0)")
#     cog.outl(f"    return x{inter_x}{inter_xwb}")
#     cog.outl("")
#
# # help function to calculate gradients for MLPs
# cog.outl("@triton.jit")
# cog.outl("def _d_linear(d_y, w, b, x):")
# cog.outl("    # gradients of `y = x @ w + b")
# cog.outl("    d_x = tl.dot(d_y, tl.trans(w), allow_tf32=ALLOW_TF32)")
# cog.outl("    d_w = tl.trans(tl.dot(tl.trans(d_y), x, allow_tf32=ALLOW_TF32))")
# cog.outl("    d_b = tl.sum(d_y, axis=0)")
# cog.outl("    return d_x, d_w, d_b")
# cog.outl("")
# cog.outl("@triton.jit")
# cog.outl("def _d_inner_product(d_y, w, b, x):")
# cog.outl("    # gradients of 'y = sum(w * x) + b")
# cog.outl("    d_w = tl.sum(x * d_y, axis=0)")
# cog.outl("    d_b = tl.sum(d_y, axis=0)")
# cog.outl("    d_x = w * d_y")
# cog.outl("    return d_x, d_w, d_b")
# cog.outl("")
# cog.outl("@triton.jit")
# cog.outl("def _d_linear_relu(d_y, w, b, xwb, x):")
# cog.outl("    # gradients of `y = max(x @ w + b, 0); xwb = x @ w + b`")
# cog.outl("    d_y_relu = d_y * (xwb > 0.0).to(tl.float32)")
# cog.outl("    return _d_linear(d_y_relu, w, b, x)")
# cog.outl("")
#
# def create_d_mlp_def(mlp_name, n_layers):
#     wb_str = cog_util.get_wb_str(mlp_name, n_layers)
#     xwb_str = cog_util.get_xwb_str(mlp_name, n_layers)
#     x_str = cog_util.get_x_str(mlp_name, n_layers)
#     dwb_str = cog_util.get_dwb_str(mlp_name, n_layers)
#     cog.outl("@triton.jit")
#     if xwb_str is not None:
#         cog.outl(f"def d_mlp_{mlp_name}(dy, "+ wb_str + ", "+ xwb_str + ", "+ x_str + "):")
#     else:
#         cog.outl(f"def d_mlp_{mlp_name}(dy, "+ wb_str + ", "+ x_str + "):")
#     for l in reversed(range(int(n_layers))):
#         if (mlp_name=="opacity" or mlp_name=="OPACITY") and l==int(n_layers)-1:
#             cog.outl(f"    dy, dw{l}_{mlp_name}, db{l}_{mlp_name} = _d_inner_product(dy, w{l}_{mlp_name}, b{l}_{mlp_name}, x{l}_{mlp_name})")
#         elif (l < int(n_layers) - 1) or (mlp_name == "trunk"):
#             cog.outl(f"    dy, dw{l}_{mlp_name}, db{l}_{mlp_name} = _d_linear_relu(dy, w{l}_{mlp_name}, b{l}_{mlp_name}, xwb{l}_{mlp_name}, x{l}_{mlp_name})")
#         else:
#            cog.outl(f"    dy, dw{l}_{mlp_name}, db{l}_{mlp_name} = _d_linear(dy, w{l}_{mlp_name}, b{l}_{mlp_name}, x{l}_{mlp_name})")
#     cog.outl(f"    return dy, "+ dwb_str + "")
#     cog.outl("")
#
# if N_LAYERS_TRUNK > 0:
#     create_load_mlp_params_def("trunk", N_LAYERS_TRUNK)
# create_load_mlp_params_def("opacity", N_LAYERS_OPACITY)
# create_load_mlp_params_def("color", N_LAYERS_COLOR)
#
# if N_LAYERS_TRUNK > 0:
#     create_mlp_def("trunk", N_LAYERS_TRUNK)
# create_mlp_def("opacity", N_LAYERS_OPACITY)
# create_mlp_def("color", N_LAYERS_COLOR)
#
# if N_LAYERS_TRUNK > 0:
#     create_mlp_with_inter_feat_def("trunk", N_LAYERS_TRUNK)
# create_mlp_with_inter_feat_def("opacity", N_LAYERS_OPACITY)
# create_mlp_with_inter_feat_def("color", N_LAYERS_COLOR)
#
# if N_LAYERS_TRUNK > 0:
#     create_d_mlp_def("trunk", N_LAYERS_TRUNK)
# create_d_mlp_def("opacity", N_LAYERS_OPACITY)
# create_d_mlp_def("color", N_LAYERS_COLOR)
#
# cog.outl("# the main function for loading the mlp params")
# cog.outl("@triton.jit")
# cog.outl("def load_mlp_params(")
# cog.outl("    mlp_params,")  # master ptr for the mlp params
# for var_name in ("DIM_HIDDEN", "DIM_IN", "DIM_OUT"):
#     for mlp_name in ("trunk", "opacity", "color"):
#         cog.outl(f"    {var_name}_{mlp_name.upper()},")
# cog.outl(f"    BLOCK_SIZE,")
# cog.outl("):")
#
# out_str = []
# prev_numel_str = ""
# for mlp_name, n_layers in zip(
#     ("trunk", "opacity", "color"),
#     (N_LAYERS_TRUNK, N_LAYERS_OPACITY, N_LAYERS_COLOR),
# ):
#     if mlp_name=="trunk" and N_LAYERS_TRUNK <= 0:
#         continue
#     mlp_name_cap = mlp_name.upper()
#     wb_str = cog_util.get_wb_str(mlp_name, n_layers)
#     cog.outl(f"    numel_{mlp_name}, {wb_str} = load_mlp_params_{mlp_name}(mlp_params{prev_numel_str}, DIM_IN_{mlp_name_cap}, DIM_HIDDEN_{mlp_name_cap}, DIM_OUT_{mlp_name_cap}, BLOCK_SIZE)")
#     prev_numel_str += f" + numel_{mlp_name}"
#     out_str.append(wb_str)
# cog.outl("    return " + ", ".join(out_str))
#
# def create_update_mlp_params_def(mlp_name, n_layers):
#     mlp_name_cap = mlp_name.upper()
#     prev_offs = "0"
#     cog.outl("@triton.jit")
#     cog.outl(f"def update_mlp_params_{mlp_name}(mlp_params, DIM_IN, DIM_HIDDEN, DIM_OUT, {cog_util.get_dwb_str(mlp_name, n_layers)}):")
#     for load_weight in [True, False]:
#         dim_in = "DIM_IN"
#         for l in range(int(n_layers)):  # load weights
#             dim_out = f"DIM_OUT" if (l == int(n_layers) - 1) else f"DIM_HIDDEN"
#             if load_weight:
#                 cog.outl(f"    w{l}_offs = {prev_offs}")
#                 cog.outl(f"    w{l}_numel = {dim_in} * {dim_out}")
#                 if l==int(n_layers)-1 and (mlp_name=="opacity" or mlp_name=="OPACITY"):
#                     cog.outl(f"    update_weight(mlp_params + w{l}_offs, {1}, {dim_in}, dw{l}_{mlp_name})")
#                 else:
#                     cog.outl(f"    update_weight(mlp_params + w{l}_offs, {dim_in}, {dim_out}, dw{l}_{mlp_name})")
#                 prev_offs = f"w{l}_offs + w{l}_numel"
#             else:
#                 cog.outl(f"    b{l}_offs = {prev_offs}")
#                 cog.outl(f"    b{l}_numel = {dim_out}")
#                 cog.outl(f"    update_bias(mlp_params + b{l}_offs, {dim_out}, db{l}_{mlp_name})")
#                 prev_offs = f"b{l}_offs + b{l}_numel"
#             dim_in = dim_out
#     cog.outl(f"    return {prev_offs}")
#     cog.outl("")
#
# if N_LAYERS_TRUNK > 0:
#     create_update_mlp_params_def("TRUNK", N_LAYERS_TRUNK)
# create_update_mlp_params_def("OPACITY", N_LAYERS_OPACITY)
# create_update_mlp_params_def("COLOR", N_LAYERS_COLOR)
# cog.outl("@triton.jit")
# cog.outl("def update_mlp_params(")
# cog.outl("    mlp_params,")  # master ptr for the mlp params
# for var_name in ("DIM_HIDDEN", "DIM_IN", "DIM_OUT"):
#     for mlp_name in ("trunk", "opacity", "color"):
#         cog.outl(f"    {var_name}_{mlp_name.upper()},")
# trunk = cog_util.get_dwb_str("TRUNK", N_LAYERS_TRUNK)
# opacity = cog_util.get_dwb_str("OPACITY", N_LAYERS_OPACITY)
# color = cog_util.get_dwb_str("COLOR", N_LAYERS_COLOR)
# if N_LAYERS_TRUNK > 0:
#     cog.outl(f"    {trunk},")
# cog.outl(f"    {opacity},")
# cog.outl(f"    {color},")
# cog.outl("):")
#
# # cog.outl("    shift_trunk, shift_opacity, shift_color =_caculate_mlp_shift(" )
# # for var_name in ("DIM_HIDDEN", "DIM_IN", "DIM_OUT"):
# #     for mlp_name in ("trunk", "opacity", "color"):
# #         cog.outl(f"       {var_name}_{mlp_name.upper()},")
# # cog.outl("    )")
#
# prev_numel_str = ""
# for mlp_name, n_layers in zip(
#     ("trunk", "opacity", "color"),
#     (N_LAYERS_TRUNK, N_LAYERS_OPACITY, N_LAYERS_COLOR),
# ):
#     if n_layers <= 0:
#         continue
#     mlp_name_cap = mlp_name.upper()
#     wb_str = cog_util.get_wb_str(mlp_name, n_layers)
#     cog.outl(f"    numel_{mlp_name} = update_mlp_params_{mlp_name_cap}(mlp_params{prev_numel_str}, DIM_IN_{mlp_name_cap}, DIM_HIDDEN_{mlp_name_cap}, DIM_OUT_{mlp_name_cap}, {cog_util.get_dwb_str(mlp_name_cap, n_layers)})")
#     prev_numel_str += f" + numel_{mlp_name}"
#
# def create_init_grad_def(mlp_name, n_layers):
#   dim_in = f"DIM_IN_{mlp_name}"
#   for l in range(n_layers):
#       dim_out = f"DIM_OUT_{mlp_name}" if (l == int(n_layers) - 1) else f"DIM_HIDDEN_{mlp_name}"
#       if l==int(n_layers)-1 and (mlp_name=="opacity" or mlp_name=="OPACITY"):
#           cog.outl(f"    dw{l}_{mlp_name} = tl.zeros((1, {dim_in}), dtype=tl.float32)")
#           cog.outl(f"    db{l}_{mlp_name} = tl.zeros(({dim_out},), dtype=tl.float32)")
#           cog.outl(f"    zero_w{l}_{mlp_name} = tl.zeros((1, {dim_in}), dtype=tl.float32)")
#           cog.outl(f"    zero_b{l}_{mlp_name} = tl.zeros(({dim_out},), dtype=tl.float32)")
#       else:
#           cog.outl(f"    dw{l}_{mlp_name} = tl.zeros(({dim_in}, {dim_out}), dtype=tl.float32)")
#           cog.outl(f"    db{l}_{mlp_name} = tl.zeros(({dim_out},), dtype=tl.float32)")
#           cog.outl(f"    zero_w{l}_{mlp_name} = tl.zeros(({dim_in}, {dim_out}), dtype=tl.float32)")
#           cog.outl(f"    zero_b{l}_{mlp_name} = tl.zeros(({dim_out},), dtype=tl.float32)")
#       dim_in = dim_out
#       cog.outl(" ")
#
# cog.outl("")
# cog.outl("# the main function for initialization grad and zero_grad buffer")
# cog.outl("@triton.jit")
# cog.outl("def init_mlp_params_grads(")
# for var_name in ("DIM_HIDDEN", "DIM_IN", "DIM_OUT"):
#     for mlp_name in ("trunk", "opacity", "color"):
#         cog.outl(f"    {var_name}_{mlp_name.upper()},")
# cog.outl("):")
#
# if N_LAYERS_TRUNK > 0:
#     create_init_grad_def("TRUNK", N_LAYERS_TRUNK)
# create_init_grad_def("OPACITY", N_LAYERS_OPACITY)
# create_init_grad_def("COLOR", N_LAYERS_COLOR)
# cog.outl("    return(")
# for fn in (cog_util.get_dwb_str, cog_util.get_zerowb_str):
#   for mlp_name, n_layers in zip(("TRUNK", "OPACITY", "COLOR"), (N_LAYERS_TRUNK, N_LAYERS_OPACITY,N_LAYERS_COLOR)):
#       if n_layers <= 0:
#           continue
#       value_str = fn(mlp_name, n_layers)
#       cog.outl(f"     {value_str},")
# cog.outl("     )")
# ]]]
# [[[end]]]
