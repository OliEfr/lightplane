{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate the basic usage of *Lightplane Renderer* and *Splatter*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all needed packages\n",
    "import sys\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import torch\n",
    "import lightplane\n",
    "from lightplane import LightplaneRenderer, LightplaneSplatter, LightplaneMLPSplatter, Rays\n",
    "\n",
    "# make sure we import the correct utils directory by adding the \"examples\" dir to path\n",
    "examples_dir = os.path.join(os.path.dirname(lightplane.__file__), \"..\", \"examples\")\n",
    "sys.path.insert(0, examples_dir)\n",
    "from utils.util.camera_util import generate_random_cameras, get_rays_for_extrinsic, get_sphere_cameras\n",
    "from utils.util.grid_util import random_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural 3D Field Representation\n",
    "\n",
    "*Lightplane* uses hybrid representations for rendering and splatting, where neural 3D fields are represented by the combination of 3D grids (we generalize it to 3D hash structure in paper) and tiny MLPs. \n",
    "\n",
    "The 3D grid is a list of 5-dim tensors, with shape $[ [ B, D_1, H_1, W_1, C ], [ B, D_2, H_2, W_2, C], ... [B, D_S, H_S, W_S, C] ]$, $S$ is the number of grids, $B$ is the batch size and $C$ is the feature dimension.\n",
    "\n",
    "For voxel grid, $S = 1$, and $D_1, H_1, W_1$ is the Depth, Height and width of the voxel grid.\n",
    "\n",
    "For TriPlanes, $S = 3$, and $D_1 = H_2 = W_3 = 1$, which is three planes. \n",
    "\n",
    "Easily, this design could support a mixture of arbitrary numbers voxel grids and planes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lightplane Renderer\n",
    "\n",
    "We now show how *Lightplane Renderer* works by rendering images from a feature grid corresponding to an RGB sphere.\n",
    "\n",
    "We first initialize the rendering rays via the `Rays` object, which structure contains `directions`(with shape $[N,3]$), `origins`(with shape $[N,3]$), `near`(with shape $[N]$), `far`(with shape $[N]$) and `grid_idx`(with shape $[N]$) for rays.\n",
    "\n",
    "In particularly, `grid_idx` is a tensor in `torch.long` format, indicating which `grid` this `ray` will render from. The value of `grid_idx` ranges from `[0, B)`. \n",
    "\n",
    "*Lightplane Renderer* samples `num_samples` points on the rays, and marches over them with the Emission Absorption algorithm to render the ray colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 16 # number of rendered images\n",
    "image_size = 128 # size of the rendered images\n",
    "B = 2 # number of scenes to render\n",
    "near = 1.0 # near rendering plane\n",
    "far = 5.0 # far rendering plane\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "R, T = get_sphere_cameras(\n",
    "    n_cameras=n_images,\n",
    "    elevation=30,\n",
    "    distance=2.0,\n",
    "    device=device,\n",
    ") # get N cameras on a sphere\n",
    "ray_dir, ray_org = get_rays_for_extrinsic(image_size, R, T) # get ray origins and ray directions\n",
    "\n",
    "near_t = torch.ones(n_images, image_size, image_size, device=device).view(-1) * near\n",
    "far_t = torch.ones(n_images, image_size, image_size, device=device).view(-1) * far\n",
    "\n",
    "# We use grid_idx to indicate the correspondence between rays and different grids (batch-wise).\n",
    "# grid_idx is a tensor (n_images * image_size * image_size, ), whose value range in [0, B)\n",
    "grid_idx = torch.linspace(\n",
    "    0, B-1, n_images, device=device\n",
    ").round().int()[:, None, None].repeat(1, image_size, image_size)\n",
    "\n",
    "rays = Rays(\n",
    "    directions=ray_dir.reshape(-1, 3).contiguous(),\n",
    "    origins=ray_org.reshape(-1, 3).contiguous(),\n",
    "    grid_idx=grid_idx.reshape(-1).contiguous(),\n",
    "    near=near_t.contiguous(),\n",
    "    far=far_t.contiguous(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lightplane Renderer* follows the popular practice of volumetric rendering. \n",
    "After sampling features from the provided grid-list `grid`, it regresses the points opacities and colors given sampling features and viewdirections. \n",
    "\n",
    "There are three MLPs inside the renderer: `trunk_mlp`, `opacity_mlp` and `color_mlp`:\n",
    "\n",
    "- `trunk_mlp` is the base MLP before color (`color_mlp`) and opacity (`opacity_mlp`) regression \n",
    "- `opacity_mlp` takes the outputs of `trunk_mlp` as input and regress opacities of sampling points.\n",
    "- `color_mlp` takes the outputs of `trunk_mlp` and view direction embedding as inputs, and outputs their colors. \n",
    "\n",
    "Additionally, *Renderer* could takes a seperate color grid for color regression, by inputing `color_feature_grid`.\n",
    "Using seperate color grids requires `mlp_n_layers_trunk = 0`.\n",
    "\n",
    "*Renderer* could also take `scaffold` as extra input, which voxel grid indicating the coarse occupancy of the 3D field. \n",
    "We will ommit the evaluation of MLPs when sampled scaffold entry is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters of the renderer\n",
    "num_samples = 256  # number of sampled points along each ray\n",
    "grid_chn = 16  # number of feature channels in the input grid\n",
    "\n",
    "# configuration of the rendering MLP, we use a simple linear for all MLPs\n",
    "mlp_hidden_chn = 16\n",
    "mlp_n_layers_opacity = 1  # the number of layers in decoder's opacity mlp \n",
    "mlp_n_layers_trunk = 1  # the number of layers in decoder's trunk mlp\n",
    "mlp_n_layers_color = 1  # the number of layers in decoder's color mlp\n",
    "\n",
    "# we configure the renderer to have viewpoint independent colors\n",
    "enable_direction_dependent_colors = False\n",
    "ray_embedding_num_harmonics = None\n",
    "\n",
    "renderer = LightplaneRenderer(\n",
    "    num_samples=num_samples, \n",
    "    color_chn=3,\n",
    "    grid_chn=grid_chn,\n",
    "    mlp_hidden_chn=mlp_hidden_chn,\n",
    "    mlp_n_layers_opacity=mlp_n_layers_opacity,\n",
    "    mlp_n_layers_trunk=mlp_n_layers_trunk,\n",
    "    mlp_n_layers_color=mlp_n_layers_color,\n",
    "    ray_embedding_num_harmonics=ray_embedding_num_harmonics,\n",
    "    opacity_init_bias=-1.0,  # the initial bias of the opacity MLP\n",
    "    enable_direction_dependent_colors=enable_direction_dependent_colors,\n",
    "    bg_color=1.0,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize the feature grid-list `grid` to render. It corresponds to a single $64^3$ voxel grid representing a 3D sphere with random surface colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, H, W = 64, 64, 64  # voxel grid size\n",
    "g_xyz = torch.stack(  # 3D coordinates of voxel centers\n",
    "    torch.meshgrid(\n",
    "        torch.linspace(-1, 1, D, device=device),\n",
    "        torch.linspace(-1, 1, H, device=device),\n",
    "        torch.linspace(-1, 1, W, device=device),\n",
    "    ),\n",
    "    dim=-1,\n",
    ")\n",
    "# set all voxels outside a sphere radius of 0.75 to be nearly empty, and random if inside\n",
    "inside_sphere_mask = g_xyz.norm(dim=-1) <= 0.75\n",
    "g = 20 * torch.randn(B, D, H, W, grid_chn, device=device) * inside_sphere_mask[..., None]\n",
    "\n",
    "# the grid-list `grid` is a list of tensors, each tensor of shape (B, D, H, W, grid_chn)\n",
    "grid = [g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render and display the results\n",
    "\n",
    "# Render!\n",
    "(\n",
    "    ray_length_render,\n",
    "    alpha_render,\n",
    "    feature_render,\n",
    ") = renderer(rays=rays, feature_grid=grid)\n",
    "\n",
    "# reshape the rendered colors to a pil image and display\n",
    "image_render = feature_render.reshape(n_images, image_size, image_size, 3).permute(0, 3, 1, 2)\n",
    "display(Markdown(\"## Rendered colors\"))\n",
    "display(transforms.ToPILImage()(torch.cat(image_render.unbind(), dim=2)))\n",
    "\n",
    "# convert the rendered alpha masks a pil image and display\n",
    "mask_render = alpha_render.reshape(n_images, image_size, image_size)\n",
    "display(Markdown(\"## Rendered alpha mask\"))\n",
    "display(transforms.ToPILImage()(torch.cat(mask_render.unbind(), dim=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Splatter\n",
    "\n",
    "Here we demonstrate how *Splatter* works with n_images input feature maps with shape $B\\times H \\times W \\times F$, $F$ is feature dimension. \n",
    "\n",
    "Each input feature map has corresponding camera parameters, including intrinsic and extrinsic parameters, near/far values, corresponding grid idx, and a pixel-wise mask for splatting.\n",
    "\n",
    "Similar to the ray-based design of rendering, the splatting is conducted on rays, which are from cast from pixels on input feature maps. \n",
    "We use the `Rays` class to store the information of rays.\n",
    "Importantly, we pass splatting features into `rays.encoding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features that will be splatted to the grid\n",
    "splatting_features = torch.randn(\n",
    "    n_images, image_size, image_size, grid_chn, device=device\n",
    ").view(-1, grid_chn)\n",
    "\n",
    "rays = Rays(\n",
    "    directions=(ray_dir.reshape(-1, 3)).contiguous(),\n",
    "    origins=(ray_org.reshape(-1, 3)).contiguous(),\n",
    "    grid_idx=(grid_idx).reshape(-1).contiguous(),\n",
    "    near=near_t.reshape(-1).contiguous(),\n",
    "    far=far_t.reshape(-1).contiguous(),\n",
    "    encoding=splatting_features.contiguous() # splatted features are stored inside the encoding field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lightplane Splatter* samples `num_samples` points on the rays, and splats them to the target 3D Grid.\n",
    "\n",
    "*Lightplane Splatter* can work in 2 modes (we split it into two `torch.nn.Module`): (1) without MLP and input grid (`LightplaneSplatter`); (2) with MLP and input grid (`LightplaneMLPSplatter`).\n",
    "\n",
    "In mode (1), for each point along the ray, `LightplaneSplatter` splats ray encoding to the output grid without any MLP or input grids. \n",
    "\n",
    "In mode (2), for each point along the ray, `LightplaneMLPSplatter` samples a feature from an input grid, appends ray encoding, passes through mlp, and splats to the output grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters\n",
    "D, H, W = 64, 64, 64 # grid sizes\n",
    "C = grid_chn # The feature dimension of grids. It is supposed to be the same as F (input feature dimension) in mode (2), while it could be different from F in mode (1).\n",
    "\n",
    "grid_sizes = [[B, 1, H, W, C], [B, D, 1, W, C], [B, D, H, 1, C]] # a triplane grid sizes\n",
    "\n",
    "input_grid = random_grid((1, D, H, W, grid_chn), device, requires_grad=True, is_triplane=True) # get a triplane input grid\n",
    "\n",
    "num_samples = 128 # number of sampling points along each ray\n",
    "use_input_grid = True # we use input grid\n",
    "\n",
    "input_grid_chn = grid_chn # features from input_grid would be summed to splatting features, so they have the same feature dimension.\n",
    "mlp_hidden_chn = 32 # the mlp hidden layer sizes of MLP insider *Splatter*.\n",
    "mlp_n_layers = 2 # the mlp depths of MLP insider *Splatter*.\n",
    "\n",
    "num_samples_inf = 0 # additional sampling numbers for unbounded regions\n",
    "contract_coords = False # whether or not use contract coordinates\n",
    "\n",
    "mask_out_of_bounds_samples = False # whether or not mask OOB samples.\n",
    "rays_jitter_near_far = True # jitter the sampling points \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first demonstrate the usage of *LightplaneSplatter* (mode 1), which takes splatting features and directly splates them into 3D grids without any MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize splatter\n",
    "splatter = LightplaneSplatter(\n",
    "    num_samples=num_samples,\n",
    "    grid_chn=C,\n",
    ").to(device)\n",
    "# output splatting results\n",
    "output_grid = splatter(\n",
    "    rays=rays, \n",
    "    grid_size=grid_sizes,\n",
    "    mask_out_of_bounds_samples=mask_out_of_bounds_samples,\n",
    "    rays_jitter_near_far=rays_jitter_near_far,\n",
    "    num_samples_inf=num_samples_inf,\n",
    "    contract_coords=contract_coords\n",
    ")\n",
    "print(output_grid[0].shape)\n",
    "print(output_grid[1].shape)\n",
    "print(output_grid[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then demonstrate the usage of *LightplaneMLPSplatter* (mode 2), which samples additional features from input_grid, and splate feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize splatter\n",
    "splatter = LightplaneMLPSplatter(\n",
    "    num_samples=num_samples,\n",
    "    grid_chn=C,\n",
    "    input_grid_chn=input_grid_chn,\n",
    "    mlp_hidden_chn=mlp_hidden_chn,\n",
    "    mlp_n_layers=mlp_n_layers,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output splatting results\n",
    "output_grid = splatter(\n",
    "    rays=rays, \n",
    "    grid_size=grid_sizes,\n",
    "    input_grid=input_grid, \n",
    "    mask_out_of_bounds_samples=mask_out_of_bounds_samples,\n",
    "    rays_jitter_near_far=rays_jitter_near_far,\n",
    "    num_samples_inf=num_samples_inf,\n",
    "    contract_coords=contract_coords\n",
    ")\n",
    "print(output_grid[0].shape)\n",
    "print(output_grid[1].shape)\n",
    "print(output_grid[2].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightplane",
   "language": "python",
   "name": "lightplane"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
